= RHEL AI and InstructLab: Get a taste in training your own model

:experimental: true
:imagesdir: ../assets/images
:toc: false
:numbered: true

[#intro]
== Introduction to the Lab

Thanks for taking the time to learn about and use our Red Hat AI portfolio of products at Red Hat One.

During this lab experience, you will learn how to create a fine-tuned version of an IBM Granite model for a fictional insurance company, **Parasol**. You will then deploy this newly enhanced model in our RHEL AI product. In another lab environment, you will then migrate this model to Red Hat OpenShift AI (RHOAI) to complete the RHOAI portion of the lab training. 

By the end of this RHEL AI portion of the hands-on workshop, you will know what https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux/ai[RHEL AI] is and how you can use it to provide value to our customers. You will also understand to leverage RHEL AI to improve a Large Language Model (LLM), and fine-tune it using synthetic data generation. 

=== Setting Up Your Terminals

To complete the lab you need two terminal windows.

Open two individual terminal tabs or windows on your local machine, and SSH into the demo environments:

From each terminal, enter: **{ssh_command}**, and use **{ssh_password}** for the password to login.

[#rhelai]
== What is RHEL AI?

RHEL AI consists of several core components:

. https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux/image-mode[RHEL Image Mode]
. The https://www.ibm.com/granite[Granite] Large Language Model(s)
. https://www.redhat.com/en/topics/ai/what-is-instructlab[InstructLab] for model alignment
. Support and indemnification from Red Hat

https://www.redhat.com/en/topics/ai/what-is-instructlab[InstructLab] is a fully open-source project from Red Hat, and the MIT-IBM Watson AI Lab, that introduces https://arxiv.org/abs/2403.01081[Large-scale Alignment for chatBots] (LAB). The project's innovation helps to simplify the instruction-tuning phase of LLM training. 

However, to fully understand the benefit of this project, you need to be familiar with some basic concepts of what an LLM is and the difficulty and cost associated with training a model.

[#llms]
=== What is a Large Language Model?

A large language model (LLM) is a type of artificial intelligence (AI) model that uses deep learning techniques to understand and generate human-like text based on input data. These models are designed to analyze vast amounts of text data and learn patterns, relationships, and structures within the data. They can be used for various natural language processing (NLP) tasks, such as:

* *Text classification*: Categorizing text based on its content, such as spam detection or sentiment analysis.
* *Text summarization*: Generating concise summaries of longer texts, such as news articles or research papers.
* *Machine translation*: Translating text from one language to another, such as English to French or German to Chinese.
* *Question answering*: Answering questions based on a given context or set of documents.
* *Text generation*: Creating new text that is coherent, contextually relevant, and grammatically correct, such as writing articles, stories, or even poetry.

Large language models typically have many parameters (millions to billions) that allow them to capture complex linguistic patterns and relationships in the data. They are trained on large datasets, such as books, articles, and websites, using techniques like unsupervised pre-training and supervised fine-tuning. Some popular large language models include GPT-4, Llama, and Mistral.

In summary, a large language model (LLM) is an artificial intelligence model that uses deep learning techniques to understand and generate human-like text based on input data. They are designed to analyze vast amounts of text data and learn patterns, relationships, and structures within the data, and can be used for various natural language processing tasks.

NOTE: To give you an idea of what an LLM can accomplish, the entire previous section was generated with a simple question against the foundational model you are using in this workshop.

[#how_trained]
=== How are Large Language Models trained?

Large language models (LLMs) are typically trained using deep learning techniques and large datasets. The training process involves several steps:

. *Data Collection*: A vast amount of text data is collected from various sources, such as books, articles, websites, and databases. The data may include different languages, domains, and styles to ensure the model can generalize well.
. *Pre-processing*: The raw text data is pre-processed to remove noise, inconsistencies, and irrelevant information. This may include tokenization, lowercasing, stemming, lemmatization, and encoding.
. *Tokenization*: The pre-processed text data is converted into tokens (words or subwords) that can be used as input and output to the model. Some models use byte-pair encoding (BPE) or subword segmentation to create tokens that can handle out-of-vocabulary words and maintain contextual information.
. *Pre-training*: The model is trained in an unsupervised or self-supervised manner to learn patterns and structures in the data.
. *Model Alignment*: (instruction tuning and preference tuning): The process of encoding human values and goals into large language models to make them as helpful, safe, and reliable as possible. This step is not as compute intensive as some of the other steps.

[#granite_intro]
== Introducing Granite

IBM Granite AI foundational models are cost-efficient, enterprise grade large language models. The Granite family of models are designed to provide enterprises with powerful tools for generative AI and natural language processing (NLP). These models are built to address the specific needs of businesses by leveraging IBM’s decades-long experience in AI, data security, and scalable cloud infrastructure. By focusing on trustworthy, responsible, and secure AI, the Granite models aim to help organizations deploy AI solutions that are not only powerful but also reliable and ethically aligned.

At their core, IBM Granite models are large language models that can understand, generate, and interact with human language. Like other popular LLMs (e.g. GPT, BERT), they are trained on vast amounts of text data, which allows them to perform a variety of NLP tasks, such as text summarization, translation, sentiment analysis, and answering questions. However, IBM Granite models distinguish themselves by being tailored for enterprise use cases, with features that emphasize data privacy, security, and regulatory compliance—critical aspects for industries such as healthcare, finance, and government.

[#granite_models]
=== Granite Models & Indemnification
IBM's Granite foundation models stand out not only technically, but also for the strong focus on indemnification, which plays a key role in reassuring enterprises about the safe and secure use of AI technologies. 

Indemnification refers to legal protection provided by IBM to its clients, ensuring that they are shielded from potential legal liabilities arising from the use of AI models, such as issues around intellectual property, copyright infringement, or bias in AI decision-making. 

This aspect of IBM’s Granite models is particularly important for enterprises that operate in highly regulated industries or those dealing with sensitive data, where compliance and risk mitigation are crucial.

It shows that the company stands behind the Granite models, not just from a technical perspective but also from a legal and ethical standpoint. For businesses, this translates into peace of mind. This is critical as AI models, including LLMs, may occasionally produce inaccurate, biased, or controversial outputs, which could lead to legal disputes if not properly managed.

[#instructlab_intro]
== Introducing InstructLab

At the foundation of RHEL AI is InstructLab.

InstructLab is an open source project, and its mission is to democratize AI and make enhancing an LLM more accessible to the average person. Typically, when someone wants to add new data to a model, it is a very complicated and time consuming process. A significant amount of time and resources go into collecting and preparing data for model fine-tuning, and setting up a model alignment pipeline, all of which requires data scientist expertise.

InstructLab takes a different approach.

When using the InstructLab tooling, users create yaml and markdown text files using plain English. InstructLab then takes care of the heavy lifting from there. This makes adding new knowledge into a model extremely easy. However, as you will see during this lab, creating the data and the necessary files is still a time consuming process.

=== What Differentiates InstructLab?

InstructLab leverages a taxonomy-guided synthetic data generation (SDG) process and a multi-phase tuning framework. SDG allows InstructLab to significantly reduce reliance on expensive human annotations, making contributing to a large language model easy and accessible. 

InstructLab uses an LLM during the process of synthetic data generation, the output of which is used to fine-tune the starter model. This alignment phase becomes most user’s starting point for contributing their knowledge.  Prior to the LAB technique, users typically had no direct involvement in training a LLM. I know this may sound complicated, but hang in there. You will see how easy this is to use.

[#skills_knowledge]
=== Skills and Knowledge

As you work with InstructLab, you will see the terms *_Skills_* and *_Knowledge_*. What is the difference between Skills and Knowledge? A simple analogy is to think of a skill as [.underline]#teaching someone how# to fish. Knowledge, on the other hand, is [.underline]#knowing# that the best place to catch a Bass is when the sun is setting while casting your line near the trunk of a tree along the bank.

[#getting_started]
== Getting started with InstructLab

=== Prerequisites for Running InstructLab

The systems you are using during this workshop are hosted on demo.redhat.com, our Red Hat Demo Platform. We are using the default RHEL AI image (leveraging Image Mode RHEL technology) deployed on a machine with adequate storage and 4 NVIDIA L40S GPUs.

As we go through the lab, you will gain a better understanding of disk space and GPU requirements for real world scenarios for your customers. As an example, we require 200GB of disk space just to download the models before even beginning to think about model training. 

=== Special Note for this Version of RHEL AI
To save time, we recommend running everything as `root`. 

There is a temporary UX issue where every time a command is run, the processing time is long due to an underlying process where the instructlab image is duplicated for the user. Running as root is a workaround. An enhancement will be implemented in a future release to address this issue.

NOTE: Even when running as `root` user, the first time you run the ilab command line tool will take 8-10 seconds. This is because it is creating a container that contains the ilab binaries.

To run every command as root, enter the following command:

[source,console,role=execute,subs=attributes+]
----
sudo su -
----

[#verify_ilab]
=== Verify ilab Installation

NOTE: You will be utilizing at least 2 terminal windows throughout the workshop.

In one terminal, type in the following to see if ilab is installed properly:

[source,console,role=execute,subs=attributes+]
----
ilab
----

That was quite of bit of information! Let’s do a version check by entering:

[source,console,role=execute,subs=attributes+]
----
ilab --version
----

The response text should indicate that you are running **version 0.19.3**. If you see a different version, please tell your lab proctor.

[#initialize_ilab]
=== Initializing InstructLab

With everything in place and working, it is time to initialize InstructLab. Go to your terminal and type the following command to initialize ilab.

[source,console,role=execute,subs=attributes+]
----
ilab config init
----

During the configuration, RHEL AI will detect the hardware profile we are using. Confirm it is correct by typing `Y`.

A few things happen during initialization. A taxonomy is generated, a configuration file (`config.yaml`) is created in the `/root/.config/instructlab/` directory, and the appropriate training profile is selected to be used during the fine-tuning process.

Let's take a look at this configuration file. Enter the following command:

[source,console,role=execute,subs=attributes+]
----
ilab config show
----

Within this configuration you can see all of the default settings. This file can be altered based on a customer’s needs. However, we do not want to encourage customers to adjust many of the settings in this file.

[#download]
== Download the Models from the Registry

Before you can truly get started with ilab, you will need to download some language models. In customer environments, these will be obtained from the official Red Hat container registry.

[#svc_account]
=== Creating a Service Account

=== Authenticating to the Red Hat Container Registry

In order to login to the container registry, you need a service account. To save you some time, we will provide you a username and password to use:

**Username**: 11009103|lab10

**Password Token**: eyJhbGciOiJSUzUxMiJ9.eyJzdWIiOiJhZTM4ZWYxMDZjYzE0ZDllOTA1OWFhYjI3ODM4OWYzZCJ9.fjw7qrmV4QzwkFKBFwdkxbwLHJ3wzpNY3jvghF9pMVN7XItVpLHVcFObAqbMWSZGs5RpiO8QhSRVNnGbThgsk2Y2maN3uwxkKD3Ym0ugsS-aqEjOC4-_wuuXBAfT4C_H7JmvE-lv9pIUvrZ9GuqpBhZZYB4XlkNPqm2rRLoEFIdkeTIaEU29hPzcYYdiIIOmIFesCxjNHfQiyvcIeQ8msMzoYiaWuIpdAuCo1DhejNKYRavFlCHkK4tcZk4zTXN7NlJh8n9tZG5Z9W2f1S59LZYmE5dhIcN7okI49XXL3SYDQmpfqplCpqpxS49mgSbqXk6Opjjw-Dy_LCTHM2_VmsFv_IsLYipYOCfZ1NKmxbNg3iwKn7W36oUkddhDATB7EFvJJYohN6rqoNfb-2T4_VK0Dws5QVl-7hXwdyO4lu2zdrIlO3u8NjlQILV0OXiDx8cQP11msr_XBvGlawkK8Yfzp1Zxd_1mnV4SQIyf09fkWWrWlr52cU2c-pVa3EuxhcenKBi65sclXcDNdzhsn7XCnAcLFROAeuSZ-VZYwXuAr0Y8wpeZs0FnLLFH6Ixvyjiu1FdwQJDUp4dPdh-K9g6IC3DLKoxci3D0OenXKfdZ3siUlO-6si9CGqxIAlSDdOlyGwe7jpi0yG6MHUCskRLqmfsze59etSVp033Cr88

NOTE: If you would like to create your own service account, navigate to https://access.redhat.com/terms-based-registry/ and login (SSO) to create a new service account. Follow the steps to create a new account. Once created, you can search for your newly created account by searching for your name in the search bar.

Now that you have credentials to the registry, you need to authenticate your RHEL AI machine.

From the command line, enter:

[source,console,role=execute,subs=attributes+]
----
podman login registry.redhat.io
----

Enter the login credentials as prompted. When successful,  you should see a response of `“Login Succeeded!”`

You are now ready to start downloading models.

[#dl_base_model]
=== Downloading the Base Model

Now that you have your credentials set up and ilab initialized, you can download the models that will be used throughout our lab experience.

First, let’s start with the base Granite model. For this lab, we will be using the Granite 7B starter model. 

Enter the following command:

[source,console,role=execute,subs=attributes+]
----
ilab model download --repository docker://registry.redhat.io/rhelai1/granite-7b-starter --release latest
----

This will only take a moment as we have pre-downloaded the models to your system. We want you to go through the motions so you understand the process.

Once the download completes, enter `ilab model list` into the terminal. You should see results similar to the image below in addition to the other preset models.

[source,console]
----
+-----------------------------------+---------------------+---------+
| Model Name                        | Last Modified       | Size    |
+-----------------------------------+---------------------+---------+
| models/granite-7b-starter         | 2024-09-24 14:40:57 | 12.6 GB |
+-----------------------------------+---------------------+---------+
----

[#serve_base]
=== Serve and Chat with the Base Model

When the download completes, you have a model that you can serve and chat with locally.

Enter the following command into one of the terminals to chat with the Granite 7B starter model.

[source,console,role=execute,subs=attributes+]
----
ilab model serve --model-path /root/.cache/instructlab/models/granite-7b-starter
----

It will take a moment to start up while vLLM loads the model into the GPU VRAM. When you see the following output, you will be able to continue.

[source,console]
----
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)
----

Now you will utilize your second terminal window that I previously mentioned you would need!

Once the model server is up and running, enter the following commands in the **other** terminal window in order to chat with the base Granite model you just downloaded. 

First, ensure you are running as root in this terminal window:

[source,console,role=execute,subs=attributes+]
----
sudo su -
----

Now enter the `ilab model chat` command:

[source,console,role=execute,subs=attributes+]
----
ilab model chat --model /root/.cache/instructlab/models/granite-7b-starter
----

You will know you are successful when the following appears on the screen:

[source,console]
----
╭─────────────────────────────────── system ──────────────────────────────────╮
│ Welcome to InstructLab Chat w/ GRANITE-7B-STARTER (type /h for help)        │
╰─────────────────────────────────────────────────────────────────────────────╯
>>>                                                                 [S][default]
----

At the chat prompt (`>>>`), enter:

[source,console,role=execute,subs=attributes+]
----
What is OpenShift?
----

You should see something similar to the below output.

NOTE: LLMs by nature are non-deterministic. This means that even with the same prompt input, the model will produce varying responses. So, your results may vary.

[source,console]
----
╭─────────────────────────────────────── granite-7b-starter ───────────────────────────────────────╮
│ OpenShift is an open source container application platform that automates the deployment,        │
│ scaling, and management of containerized applications. It provides a self-service interface for  │
│ developers to create, deploy, and manage their applications using a consistent and standardized  │
│ process. OpenShift includes features such as automated build and deployment, image registries,   │
│ networking, and security. It is designed to be highly scalable and flexible, allowing            │
│ organizations to quickly and easily deploy and manage their containerized applications in a      │
│ production-ready environment. OpenShift is built on Kubernetes, an open source container         │
│ orchestration platform, and is available as a containerized application platform, a virtual      │
│ machine image, or a bare metal solution.                                                         │
╰────────────────────────────────────────────────────────────────────────── elapsed 1.281 seconds ─╯
----

[#usecase]
== The Use Case

Now, let’s imagine we work for a fictional insurance company, **Parasol**.

We are an insurance claims agent and we need to know how much it might cost to repair a flux capacitor on a DeLorean (Marty McFly’s famed time travel vehicle from Back To The Future), which is a specific vehicle our company covers. 

We will input information about the DeLorean from Parasol’s collection of internal data, into our large language model that powers our company’s internal chatbot.

See, it’s not just all fun and games!

Now, let’s see what our starter model knows without any fine-tuning. Ask the model the below question in your terminal window where you have the `ilab model chat` command running. 

[source,console]
----
>>> /n
>>> How much does it cost to repair a flux capacitor?
----

As previously stated, the answers you see will vary due to the non-deterministic nature of LLMs. However, the output should indicate that the model knows, roughly, what a flux capacitor is and has a vague understanding of the DeLorean vehicle based on its knowledge of the classic hit movie. 

Back at the chat prompt (`>>>`) enter `/q` or `quit` to exit the session and go back to the main prompt.

You may also stop serving the model in the other terminal window by hitting kbd:[CTRL+C] to stop serving the model.

This model performs adequately, but you will see as you start to ask it more probing, specific questions, it will not perform as well as you'd like or expect. This is our "student" model, which is a slightly fine-tuned version of the Granite base model. The base model is essentially a "raw" version of a the Granite foundation model family. Specifically, it is not instruction-tuned, meaning it's not able to respond to instructions, or questions, well. This is by design, as it cannot be overly aligned before attempting to fine tune additional private data into the model.

With that in mind, let’s set up the classroom for our base model to learn what we have to teach it.

[#fine-tune]
== Fine-Tuning the Model for Better Results

We have the base model, but it does not have the knowledge we require in order to do our job as a claims agent. We need more information to process this claim for the Flux Capacitor on a DeLorean DMC-12! In order to get the model up to speed on all things Delorean, we need to teach it what we need it to know.  

[#prep_data]
=== Preparing the Data

[#doc_convert]
==== Document Conversion with Docling

In order to add new knowledge, RHEL AI needs the following data:

. markdown source document(s): documents with the desired data you wish to fine-tune the model with, in markdown format. These files are hosted in a private or public Git repository.
. `qna.yaml` file(s): A yaml-formatted file with select sections from the source markdown file(s), and pairs of question and answers for each section, to serve as instruction examples for the teacher model during synthetic data generation. The teacher nodel creates a larger synthetic dataset of questions and answers about your source document(s) based on your manually created Q&A examples. 

It is highly unlikely that our customers will already have their data in Markdown format. Therefore, we will work with customers to perform a data transformation to convert the data into a usable format for InstructLab. 

During this portion of the lab, you will see how to convert a PDF file into markdown, and then subsequently create a `qna.yaml` file.

There are many tools available to convert document formats. During this lab, we will introduce you to a project that we are working on in coordination with IBM that aims to be **the best** open source tool for converting documents into a usable format for large language model training: https://github.com/DS4SD/docling[**Docling**]. Docling is freely available on GitHub for download and use.

NOTE: Docling is now also embedded in RHEL AI as a part of the InstructLab pipeline. This implementation is relatively new, and we still see value in working on data preparation ahead of beginning the InstructLab process. Therefore, we will be discussing using Docling separately and manually.

**Docling** provides the following features:

. Reads popular document formats (PDF, DOCX, PPTX, XLSX, Images, HTML, AsciiDoc & Markdown) and exports to HTML, Markdown and JSON (with embedded and referenced images)
. Understands detailed page layout, reading order and recovers table structures
. Extracts metadata from the document, such as title, authors, references and language
. Includes OCR support for scanned PDFs
. Integrates easily with LLM app / RAG frameworks like LlamaIndex 🦙 and LangChain 🦜🔗
. Provides a simple and convenient CLI

Data preparation for InstructLab (as for any other type of training) will take care and time. We do not have time do go through this process within the lab experience. Instead we will move forward with converted .md content to prepare our qna.yaml file. However, we recommend trying Docling out on your own time to see how it works.

Once your documents are converted to .md, the next step in the process is to add the .md file(s) to a git repository. This repository may be private or public. Due to the time constraints of this lab, a repository is provided for you that contains the .md file for our example. Feel free to review the document and format.

[source,console]
----
https://github.com/rhai-code/fluxmd
----

[#q&a]
==== Creating the Questions and Answers

Now that we have our Markdown file in a git repository, the next step we need to take is to create a `qna.yaml` file. 

The `qna.yaml` format must include the following fields:

. `**version**`: The version of the qna.yaml file, this is the format of the file used for SDG. The value must be the number 3.
. `**created_by**`: Your GitHub username.
. `**domain**`: Specify the category of the knowledge.
. `**seed_examples**`: A collection of key/value entries.
.. `**context**`: A chunk of information taken verbatim from the source .md document. Each qna.yaml needs five context blocks and has a maximum token count of 500.
.. `**questions_and_answers**`: The parameter that holds your questions and answers. Each question and answer pair serves as examples for the teacher LLM to use when creating a larger set of Q&A about the source document(s).
... `**question**`: Specify a question about the content of the associated section of context. Each qna.yaml file needs at least three question and answer pairs per context chunk with a maximum token count of 250.
... `**answer**`: Specify the answer to the question, based on the associated context. Each qna.yaml file needs at least three question and answer pairs per context chunk with a maximum word count of 250 words.
. `**document_outline**`: Describe an overview of the document your submitting.
. `**document**`: The source of your knowledge contribution.
.. `**repo**`: The URL to your repository that holds your knowledge markdown files.
.. `**commit**`: The SHA of the commit in your repository with your knowledge markdown files.
.. `**patterns**`: A list of glob patterns specifying the markdown files in your repository. Any glob pattern that starts with *, such as *.md, must be quoted due to YAML rules. For example, *.md.

A proper `qna.yaml` file should have **5** context sections and **3** question and answer pairs for each context. Here is an example first section:

[source,yaml]
----
version: 3
domain: time_travel
created_by: Grant Shipley
seed_examples:
  - context: |
      The DeLorean DMC-12 is a sports car manufactured by John DeLorean's DeLorean Motor Company
      for the American market from 1981 to 1983. The car features gull-wing doors and a stainless-steel body.
      It gained fame for its appearance as the time machine in the "Back to the Future" film trilogy.
    questions_and_answers:
      - question: |
          When was the DeLorean manufactured?
        answer: |
          The DeLorean was manufactured from 1981 to 1983.
      - question: |
          Who manufactured the DeLorean DMC-12?
        answer: |
          The DeLorean Motor Company manufactured the DeLorean DMC-12.
      - question: |
          What type of doors does the DeLorean DMC-12 have?
        answer: |
          Gull-wing doors.
----

There is a lot to this data preparation step and we do not have time to go as deep as preferred, but we will highlight several points of important guidance about the `qna.yaml` file and .md source document(s):

. The source .md content must be in clean, valid markdown format. Ensure complex formatting such as tables have converted properly before proceeding to the `qna.yaml` generation step.
. The context sections of the `qna.yaml` is taken directly from the .md file. The sections are chosen with care, ensuring variety of content from throughout the entirety of the source documents.
. Ensure the associated Q&A avoids yes/no answers, and always stays true to the content of the context section. It should not reference information outside of that chunk of context.

Again, there is much to this. If you engage with customers and feel you need deeper data preparation training that you have not yet received, please reach out to your regional AI SSA team.

[#github_data]
=== Getting the Training Data

Now that we have learned what it is like to prepare our own dataset, we will obtain our prepared and tested data from GitHub to move forward.

The first step is to clone the repository where we have our `qna.yaml` file with our initial, manually input set of questions and answers. These questions and answers will be later augmented with synthetic data.

Enter the following commands:

[source,console,role=execute,subs=attributes+]
----
cd ~/fluxdata
git clone https://github.com/rhai-code/backToTheFuture.git
cd backToTheFuture
----

Let's see what we pulled down from the repository:

[source,console,role=execute,subs=attributes+]
----
ls -al
----

You should see the following:

[source,console]
----
total 20
drwxr-xr-x. 3 root root   84 Sep 29 18:08 .
drwxr-xr-x. 6 root root   73 Sep 29 18:08 ..
drwxr-xr-x. 8 root root  163 Sep 29 18:08 .git
-rw-r--r--. 1 root root  828 Sep 29 18:08 L4_x4.yaml
-rw-r--r--. 1 root root   17 Sep 29 18:08 README.md
-rw-r--r--. 1 root root 2253 Sep 29 18:08 data.md
-rwxr-xr-x. 1 root root 5166 Sep 29 18:08 qna.yaml
----

We need the `qna.yaml` file. You have just seen this file in the previous section.

Take a peek to be sure everything looks correct. Enter the following command:

[source,console,role=execute,subs=attributes+]
----
cat qna.yaml
----

As we've learned, the `qna.yaml` file consists of a list of context chunks and Q&A examples. This  will be used by the teacher model (Mixtral) to generate a larger set of synthetic data. There is also a source document which is a link to a specific commit of a text file in git, where we’ve included that a flux capacitor costs an affordable **$10,000,000**.

Now we are going to leverage the taxonomy structure to teach the starter model more detailed knowledge about the flux capacitor that we need for our insurance claims job at Parasol.

[#prepare_taxonomy]
=== Setting Up the Taxonomy

InstructLab uses a novel synthetic data-based alignment tuning method for Large Language Models (LLMs.) The "lab" in InstructLab stands for **Large-scale Alignment for chatBots**. The LAB method is driven by taxonomies, which are created manually and with care.

The way the taxonomy approach works is that the `qna.yaml` file is placed in a folder within the knowledge subdirectory of the taxonomy directory. It is placed in a folder with an appropriate name that is aligned with the data topic, as you will see in the below command.

The structure of our taxonomy directory might look something like this:

[source,console]
----
├── CODE_OF_CONDUCT.md
├── compositional_skills
│   ├── arts
│   ├── engineering
│   ├── general
│   │   └── synonyms
│   │       ├── attribution.txt
│   │       └── qna.yaml
│   ├── geography
----

To help you better understand the complete directory structure of a taxonomy, refer to the following image:

image::taxonomy.png[]

When configuring InstructLab, a taxonomy directory is set up with some sample `qna.yaml` files. These are for example for our customers. The three core most important folders are `knowledge`, `compositional_skills` and `foundational_skills` to classify our data inputs into either skill or knowledge contributions.

For our purposes, we do not need the sample files. Let's delete the preset taxonomy before creating our own:

[source,console,role=execute,subs=attributes+]
----
rm -rf /root/.local/share/instructlab/taxonomy
----

Now setup the new taxonomy directory with the folder structure needed to add our parasol data file:

[source,console,role=execute,subs=attributes+]
----
mkdir -p /root/.local/share/instructlab/taxonomy/knowledge/parasol/claims
----

Enter the following to copy the `qna.yaml` file from the GitHub repository into the correct directory in the taxonomy:

[source,console,role=execute,subs=attributes+]
----
cp ~/fluxdata/backToTheFuture/qna.yaml /root/.local/share/instructlab/taxonomy/knowledge/parasol/claims/qna.yaml
----

Verify the file was copied successfully:

[source,console,role=execute,subs=attributes+]
----
cat /root/.local/share/instructlab/taxonomy/knowledge/parasol/claims/qna.yaml | head
----

Now that your local taxonomy contains your new Parasol claims `qna.yaml` addition, you can confirm that the data addition was done correctly by entering the following command:

[source,console,role=execute,subs=attributes+]
----
ilab taxonomy diff
----

The expected response will display the following: 

[source,console]
----
knowledge/parasol/claims/qna.yaml
Taxonomy in /root/.local/share/instructlab/taxonomy is valid :)
----

With your local taxonomy data prepared, it is now time to download the other models and  tools needed for synthetic data generation, model training, and model evaluation.

[#all_models]
=== Downloading the Models Needed for Synthetic Data Generation, Training, and Evaluation

The first model to download is the **Teacher and Critic model** for the SDG (Synthetic Data Generation) phase of the training by entering the following:

[source,console,role=execute,subs=attributes+]
----
ilab model download --repository docker://registry.redhat.io/rhelai1/mixtral-8x7b-instruct-v0-1 --release latest
----

While the download may have completed instantly for you due to caching, it is important to understand the scale and size of the models that we are dealing with as part of RHEL AI. Issue the following command:

[source,console,role=execute,subs=attributes+]
----
df -h ~/.cache/instructlab/
----

You can see that we are already consuming close to 250GB of disk space just for the models. If you have used the InstructLab project on your laptop, this might be surprising to you. Keep in mind, we are using unquantized models and working directly with .safetensors instead of .gguf formatted files. This provides the highest quality models possible at the end of our fine-tuning process.

Next we will download two additional artifacts required for SDG:

**LoRA layered skills adapter**:

[source,console,role=execute,subs=attributes+]
----
ilab model download --repository docker://registry.redhat.io/rhelai1/skills-adapter-v3 --release latest
----

**LoRA layered knowledge adapter**:

[source,console,role=execute,subs=attributes+]
----
ilab model download --repository docker://registry.redhat.io/rhelai1/knowledge-adapter-v3 --release latest
----

Finally, we will download the **Judge model** for multi-phase training and evaluation with this command:

[source,console,role=execute,subs=attributes+]
----
ilab model download --repository docker://registry.redhat.io/rhelai1/prometheus-8x7b-v2-0 --release latest 
----

Enter `ilab model list` to see the downloaded models. The two LoRA adapters will not display in this command as they are not models, but are each a layer used for our Mixtral model to enhance the SDG process. 

[source,console,role=execute,subs=attributes+]
----
ilab model list
----

[source,console]
----
+-----------------------------------+---------------------+---------+
| Model Name                        | Last Modified       | Size    |
+-----------------------------------+---------------------+---------+
| models/granite-7b-starter         | 2024-09-24 14:40:57 | 12.6 GB |
| models/mixtral-8x7b-instruct-v0-1 | 2024-09-24 15:05:43 | 87.0 GB |
| models/prometheus-8x7b-v2-0       | 2024-09-24 15:20:05 | 87.0 GB |
+-----------------------------------+---------------------+---------+
----

The skills and knowledge adapters can be found in the `/root/.cache/instructlab/models/` directory.

Run the following command:

[source,console,role=execute,subs=attributes+]
----
ls -al /root/.cache/instructlab/models/
----

You will see all five in this directory.

[source,console]
----
drwxr-xr-x. 2 root root 4096 Sep 24 14:40 granite-7b-starter
drwxr-xr-x. 2 root root 4096 Sep 24 15:23 knowledge-adapter-v3
drwxr-xr-x. 2 root root 4096 Sep 24 15:05 mixtral-8x7b-instruct-v0-1
drwxr-xr-x. 2 root root 4096 Sep 24 15:20 prometheus-8x7b-v2-0
drwxr-xr-x. 2 root root 4096 Sep 24 15:23 skills-adapter-v3
----

For a more detailed description of the models, please refer to the https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_ai/1.1/html/building_your_rhel_ai_environment/downloading_ad_models#downloading_ad_models[RHEL AI documentation].

[#sdg]
== Generating Synthetic Data

Now, let’s move on to the innovative component that sets InstructLab apart from other methods of fine-tuning. With our synthetic data generation pipeline, the InstructLab tooling uses the structure of our taxonomy and the addition of our `qna.yaml` file to generate a large synthetic dataset. This large dataset is required to impact our LLM effectively. The teacher model, Mixtral, assists in this process both in generating the new examples and pruning the dataset for inaccuracies or duplications. 

For this workshop, we are showing you the optimal experience we expect customers to use for production. With a nicely-specced enterprise-grade GPU-accelerated machine the synthetic generation step takes around 10-15 minutes.  These are the machines we have selected for this workshop.

We will now run the command to generate synthetic data. 

NOTE: If either terminal is still serving the Granite model or running a process, kill that process by entering kbd:[CTRL+C]. The data generation will fail if a model is running.

Enter the following command:

[source,console,role=execute,subs=attributes+]
----
ilab data generate
----

Do not be alarmed if you see a message similar to the following:

[source,console]
----
INFO 2024-09-27 02:09:38,203 instructlab.model.backends.backends:480: Waiting for the vLLM server to start at http://127.0.0.1:33399/v1, this might take a moment... Attempt: 15/120
----

Eventually, the vLLM server will start and the synthetic data generation will begin.

You will begin to see InstructLab is now synthetically generating examples based on the seed data you provided in the `qna.yaml` file. You will see output on your screen indicating the data is being generated:

[source,console]
----
INFO 2024-09-28 03:12:55,518 instructlab.model.backends.backends:487: vLLM engine successfully started at http://127.0.0.1:37211/v1
Generating synthetic data using '/usr/share/instructlab/sdg/pipelines/agentic' pipeline, '/root/.cache/instructlab/models/mixtral-8x7b-instruct-v0-1' model, '/root/.local/share/instructlab/taxonomy' taxonomy, against http://127.0.0.1:37211/v1 server
INFO 2024-09-28 03:12:55,974 instructlab.sdg:375: Synthesizing new instructions. If you aren't satisfied with the generated instructions, interrupt training (Ctrl-C) and try adjusting your YAML files. Adding more examples may help.
INFO 2024-09-28 03:12:56,071 instructlab.sdg.checkpointing:59: No existing checkpoints found in /root/.local/share/instructlab/datasets/checkpoints/knowledge_parasol_claims, generating from scratch
INFO 2024-09-28 03:12:56,072 instructlab.sdg.pipeline:158: Running pipeline with multi-threaded batching. Using 10 workers for batches of size 8
INFO 2024-09-28 03:12:56,157 instructlab.sdg.llmblock:51: LLM server supports batched inputs: True
INFO 2024-09-28 03:12:56,157 instructlab.sdg.pipeline:197: Running block: router
INFO 2024-09-28 03:12:56,157 instructlab.sdg.pipeline:198: Dataset({
    features: ['icl_document', 'document', 'document_outline', 'domain', 'icl_query_1', 'icl_query_2', 'icl_query_3', 'icl_response_1', 'icl_response_2', 'icl_response_3'],
    num_rows: 5
})
----

Once the synthetic data generation completes you will see a message similar to the following and be returned to the prompt.

[source,console]
----
INFO 2024-09-28 03:17:58,069 instructlab.sdg:438: Generation took 302.55s
INFO 2024-09-28 03:18:04,395 instructlab.model.backends.backends:351: Waiting for GPU VRAM reclamation...
----

[#view_sd]
=== [Optional] Viewing your New Synthetic Dataset

Let’s take a look at the generated data. The SDG process creates a JSONL file located at 

  /root/.local/share/instructlab/datasets/

The file name format is: 

  knowledge_train_msgs_[TIMESTAMP].jsonl

The exact file name is shown in the finishing output of the `ilab data generate` command. You can find your exact synthetic knowledge data file in the `/root/.local/share/instructlab/datasets` folder, among other file outputs.

Enter the following command to see the contents of the JSONL file. Be sure to adjust what you enter into the terminal based on the timestamp of your JSONL file. 

[source,console, role=execute, subs=attributes+]
----
cat /root/.local/share/instructlab/datasets/knowledge_train_msgs_[TIMESTAMP].jsonl
----

This file contains the synthetically generated data for us to now fine-tune our model with. The contents are challenging to read through, particularly if we were trying to validate and edit the content manually. 

Luckily, we have written a small python script to parse the contents of a SDG file. To inspect the synthetic data with the **sdgparser**, enter in the following commands:

[source,console, role=execute, subs=attributes+]
----
cd ~/fluxdata
git clone https://github.com/rhai-code/sdgparser.git
cd sdgparser
python sdgparse.py ~/.local/share/instructlab/datasets/knowledge_train_msgs_[TIMESTAMP.jsonl |less
----

For example, you would enter in the following command to parse the contents of a file named `knowledge_train_msgs_2024-09-29T19_03_56.jsonl`

[source,console, role=execute, subs=attributes+]
----
python sdgparse.py ~/.local/share/instructlab/datasets/knowledge_train_msgs_2024-09-29T19_03_56.jsonl |less
----

You should see output similar to the following:

[source,console]
----
Processing file: /root/.local/share/instructlab/datasets/knowledge_train_msgs_2024-09-29T19_03_56.jsonl
Question: What type of engine does the DeLorean DMC-12 have?.
Answer: The DeLorean DMC-12 has a 2.85 L V6 PRV engine.

Question: What are the two transmission options for the DeLorean DMC-12?.
Answer: The two transmission options for the DeLorean DMC-12 are a 5-speed manual and a 3-speed automatic.

Question: What is the 0-60 mph acceleration time for the DeLorean DMC-12?.
Answer: The 0-60 mph acceleration time for the DeLorean DMC-12 is approximately 8.8 seconds.

Question: What is the top speed of the DeLorean DMC-12?.
Answer: The top speed of the DeLorean DMC-12 is 110 mph.

Question: What is the weight of the DeLorean DMC-12?.
Answer: The weight of the DeLorean DMC-12 is 2,712 lb (1,230 kg).

Question: What is the regular maintenance schedule for the DeLorean DMC-12?.
Answer: The regular maintenance schedule for the DeLorean DMC-12 includes regular oil changes every 3,000 miles or 3 months, brake fluid change every 2 years, transmission fluid change every 30,000 miles, coolant change every 2 years, and regular battery checks for corrosion and proper connection. The flux capacitor requires regular fluid addition.

Question: What are some common repairs for the DeLorean DMC-12?.
Answer: Some common repairs for the DeLorean DMC-12 include engine rebuilds

Question: What is the horsepower and torque of the DeLorean DMC-12?.
Answer: The DeLorean DMC-12 has a horsepower of 130 and a torque of 153 lb-ft.

Question: What is the weight of the DeLorean DMC-12?.
Answer: The DeLorean DMC-12 weighs around 2,712 lb (1,230 kg).

Question: What is the cost of repairing the engine on a DeLorean DMC-12?.
Answer: An engine rebuild for a DeLorean DMC-12 costs between $5,000 to $7,000.

......
----

Pretty neat!

NOTE: The parser is not officially provided from engineering, we created this for our training session. Thus, it is imperfect.

Now, let’s use this large synthetic dataset to fine-tune the model.  

[#ft_model]
== Fine-Tuning the Model

Training using the newly generated data is a time and resource intensive task. Depending on the number of epochs desired, internet connection for safetensor downloading, and other factors, it can take hours or days to really fine tune the model. It is not required to train the model to continue with the lab as we will use an already trained model.

[#one_epoch]
=== Running One Epoch

Due to the time constraints of this lab, we are not able to fully fine-tune our model. While a fully tuned and trained model will be provided to you, we want you to understand what happens during a real training process.

The best way to illustrate this is by running one **epoch** of training. An epoch in machine learning, in the context of training a model, refers to one complete pass through the entire dataset.

The following command will perform one epoch of fine-tuning. It should take several minutes to complete. Once again, be sure to change the exact filename to reflect the timestamp of your model.  It will be the same JSONL file referenced earlier. Just as before, make sure that you are not serving a model in either of the terminals. We will need all the GPU memory we have.

[source,console, role=execute, subs=attributes+]
----
ilab model train --data-path  /root/.local/share/instructlab/datasets/knowledge_train_msgs_[timestamp].jsonl --num-epochs 1 --device=cuda
----

In several minutes, you should see the following which indicates that the training is complete.

[source,console]
----
Model saved in /root/.local/share/instructlab/checkpoints/hf_format/samples_192
[14:11:51] INFO     saving took 20.465868711471558 seconds                                 utils.py:611
Epoch 0: 100%|██████████| 2/2 [00:56<00:00, 28.34s/it]
Operation completed successfully! 🎉
----

Type in the following command to see the model files that were just created:

[source,console, role=execute, subs=attributes+]
----
ls -al /root/.local/share/instructlab/checkpoints/hf_format/samples_192
----

=== Running a One Epoch Trained Model

By now, you have likely heard how long it takes to train a language model. Yet, we just did it in about five minutes. If you’re wondering what the catch is, let’s chat with the model and see how it performs. The answer will be obvious.

In one of the terminals, enter the following command to serve the model we just created:

[source,console, role=execute, subs=attributes+]
----
ilab model serve --model-path /root/.local/share/instructlab/checkpoints/hf_format/samples_192
----

Once vLLM has started and is accepting requests, in the other terminal, enter the following to chat with the newly-trained model:

[source,console, role=execute, subs=attributes+]
----
ilab model chat -gm --model /root/.local/share/instructlab/checkpoints/hf_format/samples_192
----

You then should see the following response indicating that you are in a chat session:

[source,console]
----
╭─────────────────────────────────────────────── system ───────────────────────────────────────╮
│ Welcome to InstructLab Chat w/ SAMPLES_192 (type /h for help)                                │
╰──────────────────────────────────────────────────────────────────────────────────────────────╯
----

Ask the model:

  >>> How much does it cost to replace a flux capacitor in a DeLorean DMC-12 in millions of dollars?

You will see that the accuracy has not improved much, if at all.

Exit the chat by entering `/q` or `quit` and in the other terminal, enter kbd:[CTRL-C] to exit the model server.

Clearly, one epoch won’t cut it. 

Let’s take inspiration from another 1980s classic movie and turn it up to eleven!

[#full_train]
=== Fully Training the Model

In a production environment, a short, incomplete training run would not work of course. During this workshop, we do not have time available to experience the joy and excitement of waiting for a full training process to execute. We will give you a fully-trained model, ready to go, for the remainder of this lab. 

However, we want you to understand, at a high level, what a “full training process” is as this is what customers should be running. 

Our RHEL AI training is a multi-phase training strategy. There are two phases:

* **Phase 1**: We take the starter model (granite-7b-starter), and fine-tune it with the synthetic knowledge dataset produced from SDG. This training process outputs a model ‘checkpoint’ after each epoch of training. The checkpoints are evaluated by Prometheus using standard LLM benchmarks. The best model checkpoint goes to Phase 2.

* **Phase 2**: Our model output from phase 1 is now trained on skills data. This skills data is based on data pre-loaded into the RHEL AI product and is focused on ensuring our final model can process, understand, and answer in the most accurate way from the given context. This process outputs new checkpoints for each epoch that runs. The checkpoints can at this point be evaluated by the customer and the ‘best’ one becomes our final model.

The time this takes varies based on infrastructure. Check out our https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_ai/1.1/html/creating_a_custom_llm_using_rhel_ai/train_and_eval#training_process[official documentation] to review the command line process for training. 

Now, let’s try the model we have trained for you.

[#fully_trained_model]
== Try the Fully Trained Model

Our fully trained model is cached alongside our other models in the `/root/.cache/instructlab/models` directory.

Let's serve and chat with our fully trained model as we've done before:

[source,console, role=execute, subs=attributes+]
----
ilab model serve --model-path /root/.cache/instructlab/models/thegshipley/fluxcapacitor
----

In the other terminal window, issue the `chat` command once again:

[source,console, role=execute, subs=attributes+]
----
ilab model chat --model /root/.cache/instructlab/models/thegshipley/fluxcapacitor
----

Test it out with the same questions. What do you think? 

[#conclusion]
== Conclusion

Great job! 

The goal here was to get you familiar with the process and hardware needed to fine-tune a LLM via the LAB method and RHEL AI. In that regard, mission accomplished! 

Now that you have gotten a taste of AI engineering, you're likely curious about where to go next. 

The next step in your RH One lab journey is to take the model and serve it from RHOAI as part of a larger application. The following lab will guide you through those steps.

As a call to action, as you embark on your year, begin to think about how RHEL AI trained models can help your customers achieve their business goals.

As you can see, InstructLab is pretty straightforward. Most of the time you spend will be on curating new model training data. Again, we’re so happy you made it this far, and remember if you have questions we are here to help, and are excited to see what you come up with!

Learn more about RHEL AI here by reading our https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_ai/[official documentation] and feel free to reach out to the Red Hat AI Technical Marketing Team to answer any questions that may come up.

Now let's go to Red Hat OpenShift AI! 